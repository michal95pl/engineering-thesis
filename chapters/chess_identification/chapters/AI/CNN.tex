\setcounter{chapter}{7}
\chapter[Architektura sieci neuronowej do klasyfikacji bierek {[\textit{Marcin Ziółkowski}]}]{Architektura sieci neuronowej do klasyfikacji bierek}

Współczesne systemy wizyjne opierają swoją skuteczność na splotowych sieciach neuronowych (ang. \textit{Convolutional Neural Networks} – CNN). W przeciwieństwie do klasycznych sieci gęstych (MLP), sieci CNN są zaprojektowane tak, aby wykorzystywać przestrzenną strukturę danych obrazowych poprzez mechanizm współdzielenia wag i lokalnej spójności.

\section{Teoretyczne podstawy działania sieci CNN}

Jak było dokładnie opisane w rozdziale 3, fundamentem działania warstwy konwolucyjnej jest operacja splotu. Polega ona na przesuwaniu małej macierzy wag, zwanej filtrem lub jądrem (ang. \textit{kernel}), nad obrazem wejściowym. Dzięki temu sieć jest w stanie wykrywać wzorce niezależnie od ich położenia w polu widzenia.

\vspace{0.5cm}

Fakty techniczne dotyczące ekstrakcji cech w modelu \texttt{ChessCNN}:
\begin{itemize}
    \item \textbf{Hierarchia cech:} Pierwsze warstwy splotowe uczą się rozpoznawać proste krawędzie i kontury symboli na tokenach. Warstwy głębsze identyfikują złożone piktogramy, takie jak charakterystyczne zwieńczenie korony hetmana czy krzywizny piktogramu skoczka.
    \item \textbf{Inwariantność przestrzenna:} Dzięki zastosowaniu warstw \texttt{MaxPool2d}, sieć staje się odporna na niewielkie przesunięcia tokenu względem środka pola szachownicy.
    \item \textbf{Normalizacja wsadowa (Batch Normalization):} Stabilizuje proces uczenia poprzez normalizację aktywacji, co zapobiega problemowi zanikającego gradientu i pozwala na stosowanie wyższych współczynników uczenia (\textit{learning rate}). Warstwa ta również została szczegółowo omówiona w rozdziale 3.
\end{itemize}

\newpage

\section{Szczegółowa architektura ChessCNN}

Model \texttt{ChessCNN} został zaprojektowany z myślą o efektywności obliczeniowej oraz maksymalnym wykorzystaniu lokalnych cech obrazu o niskiej rozdzielczości ($30 \times 30$ pikseli). Architektura opiera się na trzech blokach ekstrakcji cech, z których każdy realizuje specyficzne zadanie w procesie interpretacji piktogramu na tokenie.



\subsection{Bloki splotowe i ekstrakcja cech}

W modelu zastosowano strukturę podwójnych warstw splotowych przed każdą operacją próbkowania w dół (ang. \textit{downsampling}). Takie podejście, inspirowane architekturą VGG, pozwala na uzyskanie większego pola recepcyjnego przy mniejszej liczbie parametrów w porównaniu do pojedynczych warstw z większymi filtrami (np. $5 \times 5$).

\begin{itemize}
    \item \textbf{Blok 1 (Niskopoziomowy):} Składa się z dwóch warstw \texttt{Conv2d} po 32 filtry każda. Na tym etapie sieć skupia się na detekcji pierwotnych cech wizualnych, takich jak krawędzie symbolu oraz kontrast między czarnym nadrukiem piktogramu a białym/beżowym tłem tokenu.
    \item \textbf{Blok 2 (Średniopoziomowy):} Zwiększa liczbę filtrów do 64. Tutaj następuje synteza krawędzi w proste kształty geometryczne – łuki, kąty ostre i przecięcia linii, które zaczynają formować unikalne sygnatury figur (np. ząbkowanie wieży).
    \item \textbf{Blok 3 (Wysokopoziomowy):} Wykorzystuje 128 filtrów. Reprezentacje stają się tutaj na tyle abstrakcyjne, że pojedyncze mapy cech odpowiadają za detekcję całych komponentów symbolu, takich jak "głowa skoczka" czy "podstawa pionka".
\end{itemize}

\subsection{Redukcja wymiarowości i Global Average Pooling}

Kluczowym elementem architektury jest przejście z przestrzennych map cech do wektora klasyfikacyjnego. W tradycyjnych podejściach stosowano warstwy \texttt{Flatten} bezpośrednio po ostatnim splotcie, co generowało tysiące połączeń. W \texttt{ChessCNN} zastosowano nowoczesne podejście \textbf{Global Average Pooling (GAP)}.



Operacja \texttt{AdaptiveAvgPool2d(1)} uśrednia każdą ze 128 map cech o wymiarze $7 \times 7$ do pojedynczej wartości. Ma to trzy główne zalety:
\begin{enumerate}
    \item \textbf{Redukcja parametrów:} Liczba wag w warstwie klasyfikatora zależy tylko od liczby filtrów w ostatnim bloku, a nie od rozdzielczości obrazu.
    \item \textbf{Odporność na przesunięcia:} Uśrednianie sprawia, że wynik jest inwariantny względem dokładnego położenia aktywowanej cechy w polu widzenia.
    \item \textbf{Interpretowalność:} Każda z 128 wartości w końcowym wektorze reprezentuje siłę występowania konkretnego wzorca w całym obrazie pola.
\end{enumerate}

\newpage

\subsection{Klasyfikator i regularyzacja}

Ostatnim etapem jest dwuwarstwowy klasyfikator. Zastosowanie warstwy \texttt{Dropout} z parametrem $0.2$ przed warstwą liniową pełni rolę regularyzatora. 

\begin{equation}
    y = \text{softmax}(W \cdot (\text{dropout}(f)) + b)
\end{equation}

Podczas treningu losowo wyłączane jest 20\% neuronów, co wymusza na sieci tworzenie redundantnych ścieżek przepływu informacji i zapobiega sytuacji, w której model "zapamiętuje" konkretne szumy obecne na zdjęciach treningowych (overfitting). Finalna warstwa \texttt{Linear} mapuje 128 cech na 6 klas bierek, a funkcja \texttt{LogSoftmax} (zaszyta w \texttt{CrossEntropyLoss}) zwraca rozkład prawdopodobieństwa dla każdej klasy.

\begin{table}[h]
\centering
\caption{Podsumowanie przepływu danych przez model ChessCNN}
\begin{tabular}{|l|c|r|}
\hline
\textbf{Warstwa} & \textbf{Rozmiar wyjściowy} & \textbf{Liczba parametrów} \\ \hline
Wejście (RGB) & $30 \times 30 \times 3$ & 0 \\ \hline
Blok 1 (2x Conv32) & $15 \times 15 \times 32$ & ~10,000 \\ \hline
Blok 2 (2x Conv64) & $7 \times 7 \times 64$ & ~55,000 \\ \hline
Blok 3 (2x Conv128) & $7 \times 7 \times 128$ & ~220,000 \\ \hline
Global Avg Pool & $1 \times 1 \times 128$ & 0 \\ \hline
Linear Classifier & 6 klas & ~800 \\ \hline
\end{tabular}
\end{table}

\section{Strategia uczenia i obsługa nierównowagi klas}

Jednym z najpoważniejszych wyzwań w projektowaniu systemów klasyfikacji obrazów jest nierównowaga klas (ang. \textit{class imbalance}), szczególnie w szachach, gdzie w standardowej partii stosunek liczby pionów do królów wynosi 8:1. 

Bez odpowiedniej strategii, sieć neuronowa dążąc do minimalizacji globalnej funkcji straty, mogłaby popaść w stan "lenistwa statystycznego", klasyfikując większość obiektów jako piony i wciąż osiągając relatywnie wysoką dokładność, mimo całkowitej bezużyteczności w rozpoznawaniu kluczowych figur.

\subsection{Ważona funkcja kosztu Cross-Entropy}

Aby przeciwdziałać temu zjawisku, zastosowano ważoną postać funkcji kosztu \textit{Cross-Entropy}. Matematyczna definicja straty dla pojedynczej próbki $x$ z etykietą $y$ przyjmuje postać:

\begin{equation}
    L(x, y) = -w_{y} \log\left( \frac{e^{s_y}}{\sum_{j} e^{s_j}} \right)
\end{equation}

gdzie $s_y$ jest wynikiem (ang. \textit{logit}) zwróconym przez sieć dla poprawnej klasy, a $w_y$ jest przypisaną jej wagą. Wagi zostały obliczone jako odwrotność częstości występowania danej klasy w zbiorze treningowym:

\newpage

\begin{lstlisting}[style=codeListingStyle, caption={Implementacja wag klas w procesie treningowym}]
# Obliczenie liczebnosci klas
piece_cnt = collections.Counter([s[1] for s in dataset.samples])

# Wyznaczenie wag - im rzadsza klasa, tym wyzsza waga
piece_weights = 1. / torch.tensor([piece_cnt[p] for p in PIECES], dtype=torch.float32)

# Normalizacja wag (opcjonalna, ale zalecana dla stabilnosci)
piece_weights = piece_weights / piece_weights.sum() * len(PIECES)

criterion = torch.nn.CrossEntropyLoss(weight=piece_weights.to(device))
\end{lstlisting}

Dzięki temu zabiegowi, błąd popełniony przy rozpoznawaniu Króla generuje znacznie silniejszy sygnał gradientu niż błąd przy Pionie, co wymusza na optymalizatorze \texttt{AdamW} znalezienie takich wag modelu, które precyzyjnie oddzielają rzadkie klasy od reszty zbioru.

\subsection{Optymalizacja i stabilizacja gradientu}

W procesie uczenia wykorzystano optymalizator \texttt{AdamW} (Adam z poprawionym \textit{Weight Decay}), który łączy zalety adaptacyjnego współczynnika uczenia z poprawną regularyzacją $L_2$. 

\begin{itemize}
    \item \textbf{Weight Decay ($10^{-2}$):} Zapobiega nadmiernemu wzrostowi wartości wag, co jest kluczowe przy małych zbiorach danych (np. tokeny szachowe), chroniąc model przed overfittingiem.
    \item \textbf{Cosine Annealing:} Zastosowanie harmonogramu cosinusoidalnego pozwoliło na "łagodne" lądowanie w lokalnym minimum funkcji straty. W początkowych fazach wysoki parametr $LR$ pozwalał na szybką eksplorację przestrzeni wag, natomiast w końcowych fazach (okolice 10-12 epoki) drastyczne zmniejszenie $LR$ pozwoliło na precyzyjne dostrojenie modelu.
\end{itemize}



Taka kombinacja – ważona funkcja kosztu oraz zaawansowany harmonogram uczenia – pozwoliła na uzyskanie modelu, który mimo wrażliwości na cienie i odblaski, wykazuje się wysoką czułością (ang. \textit{recall}) dla wszystkich figur, niezależnie od ich liczebności w zestawie.

\newpage

\section{Analiza wyników i wrażliwość modelu}

Proces uczenia trwał 12 epok z wykorzystaniem \texttt{CosineAnnealingLR}. Pomimo osiągnięcia wysokiej dokładności w warunkach testowych, model wykazuje wrażliwość na czynniki zewnętrzne charakterystyczne dla fizycznych tokenów.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{chapters/chess_identification/images/training_chart.png}
    \caption{Przebieg funkcji straty oraz dokładności – stabilizacja dzięki harmonogramowi cosinusoidalnemu.}
\end{figure}

Głównym problemem są cienie i odbicia światła na powierzchni tokenów. Ponieważ model operuje na dwuwymiarowych symbolach, silne odblaski mogą zniekształcić kontury piktogramu, prowadząc do błędnej klasyfikacji (np. mylenia Gońca ze Skoczkiem, jeśli odblask "odcina" fragment piktogramu). Aby zminimalizować ten efekt, w procesie uczenia wykorzystano techniki augmentacji (biblioteka \texttt{Albumentations}), takie jak zmiany jasności, kontrastu oraz dodawanie szumu Gaussa, co ma na celu symulację trudnych warunków oświetleniowych panujących w rzeczywistym środowisku rozgrywki.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{chapters/chess_identification/images/Matrix.png}
    \caption{Macierz pomyłek modelu ChessCNN. Widoczne skupienie najwyższych wartości na przekątnej.}
\end{figure}