\setcounter{chapter}{1}

\chapter{Algorytm Monte Carlo Tree Search}
Zadaniem algorytmu w szachach jest znalezienie najlepszego ruchu w danej pozycji poprzez przewidywanie strategii przeciwnika. Słowo "strategia" jest kluczowe, gdyż odnosi się do planowania rozgrywki w przód, w celu realizacji zamierzeń w przyszłości, a nie tylko w obecnym stanie. Do wykonania tego zadania służą algorytmy drzewiaste analizujące różne kombinacje ruchów graczy. Takim algorytmem jest Monte Carlo Tree Search, który w połączeniu z siecią neuronową jest bardzo skutecznym algorytmem do gry w szachy.

\section{Podstawowa wersja MCTS}
Algorytm Monte Carlo Tree Search jest heurystycznym algorytmem przeszukiwania przestrzeni stanów. Jego kluczową zaletą jest iteracyjność, dzięki czemu w zależności od dostępnej mocy obliczeniowej (czasu) jest w stanie stworzyć dowolnie duże drzewo, gdzie po każdej iteracji jest gotowy zwrócić najlepszy dotychczasowy ruch. Działanie tego algorytmu opiera się na czterech etapach: selekcji, ekspansji, symulacji oraz wstecznej propagacji.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\textwidth]{images/mcts_sections.png}
\caption{Etapy algorytmu Monte Carlo Tree Search. Źródło: GeeksforGeeks, Monte Carlo Tree Search (MCTS) in Machine Learning}
\end{figure}

\newpage

\subsection{Selekcja}
Podczas etapu selekcji algorytm wybiera najlepszą ścieżkę od korzenia drzewa do liścia poprzez wybór odpowiednich węzłów. Są one wybierane na podstawie funkcji UCT:

\begin{equation}
\operatorname{UCT}(s,a) \,=\, \widehat{Q}(s,a) \, + \, c\, \sqrt{\frac{\ln(N(s))}{N(s,a)}},\,
\quad
\widehat{Q}(s,a) \,=\, \dfrac{W(s,a)}{N(s,a)}\,
\end{equation}

\noindent gdzie:
\begin{description}
  \item[$N(s)$] - liczba odwiedzin stanu $s$
  \item[$N(s,a)$] - liczba odwiedzeń akcji $a$ w stanie $s$
  \item[$W(s,a)$] - wartość sumy nagród akcji $a$ w stanie $s$
  \item[$c$] - współczynnik eksploracji
\end{description}

\hspace{1cm}

Wzór ten składa się z sumy dwóch składników. Pierwszy z nich $\widehat{Q}(s,a)$ odpowiada za eksploatację, czyli wybór węzła, który jak dotąd osiągnął najlepszy wynik. Natomiast drugi $c\, \sqrt{\frac{\ln(n_s)}{n_{s,a}}}$ skupia się na eksploracji. Na rysunku 2.2 można zauważyć, że współczynnik ten znacznie bardziej faworyzuje mniejszą liczbę odwiedzin $n_{s,a}$. Ponadto użycie logarytmu naturalnego sprawia, że zmiana liczby odwiedzin węzła rodzica nie powoduje nagłych zmian. Współczynnik $c$ pozwala dostosować balans między eksploracją, a eksploatacją. W opisywanym algorytmie jest ustawiona standardowa wartość $c = \sqrt{2}$ = 1,41.

\begin{figure}[!t]
\centering
\begin{tikzpicture}
\begin{axis} [
    xlabel={Liczba odwiedzin węzła $n(s,a)$},
    ylabel={Wartość eksploracji},
    grid=major,
    width=10cm,
    height=6cm,
    domain=1:30,
    samples=200
]
\addplot[blue, thick] {sqrt(ln(50)/x)};
\legend{$\sqrt{\frac{\ln 50}{n(s,a)}}$}
\end{axis}
\end{tikzpicture}
\caption{Składnik eksploracji $\sqrt{\frac{\ln(N(s))}{N(s,a)}}$ dla $N(s)=50$}
\label{fig:uct-exploration}
\end{figure}

\newpage

\subsection{Ekspansja}
Etap ekspansji ma za zadanie rozbudowywać drzewo o kolejne węzły. Są one wybierane w sposób losowy spośród wszystkich możliwych akcji w danym stanie.

\subsection{Symulacja}
Etap symulacji polega na przeprowadzeniu symulacji rozgrywki od nowo dodanego węzła do końca gry w sposób losowy. W przypadku szachów oznacza to wykonywanie losowych ruchów aż do osiągnięcia stanu końcowego: mat, pat, remis.

\subsection{Wsteczna propagacja}
Ostatnim krokiem w algorytmie jest wsteczna propagacja. Ma ona na celu zaktualizowanie statystyk węzłów znajdujących się na wybranej ścieżce. W każdym z nich jest modyfikowana liczba odwiedzin oraz wartość zwrócona z symulacji.

\section{Połączenie MCTS z siecią neuronową}
Algorytm Monte Carlo Tree Search w podstawowej wersji korzysta z naiwnej metody oceniającej obecny stan gry. W grach takich jak kółko i krzyżyk, gdzie przestrzeń stanów jest bardzo ograniczona, jest to wystarczające rozwiązanie i przy odpowiedniej liczbie iteracji skuteczne. Natomiast w bardziej złożonych, jak przedstawiane szachy, taki sposób oceny staje się niewystarczający. Rozwiązaniem jest połączenie MCTS z siecią neuronową, która będzie w stanie ocenić obecny stan gry oraz przedstawić rozkład prawdopodobieństwa wszystkich następnych ruchów. Takie podejście wymaga stworzenia dwóch oddzielnych sieci, lub jednej z dwoma grupami wyjść, gdzie jedna z nich będzie siecią policy, a druga value.

W celu integracji MCTS z siecią neuronową, funkcja UCT została zastąpiona PUCT (Predictor + UCT). Takie samo podejście zostało zastosowane w AlphaZero \cite{AlphaZero}. Różnica między UCT a PUCT występuje w obliczaniu eksploracji, gdzie między innymi jest uwzględniane prawdopodobieństwo ruchu $P(s,a)$ zwracane przez sieć policy. Dodatkowo informacja z sieci value jest wstecznie propagowana w drzewie, zastępując prostą nagrodę z symulacji.
\hspace{2cm}

\begin{equation}
\operatorname{PUCT}(s,a) \,=\, \widehat{Q}(s,a) \, + \, c_{\mathrm{puct}}\, P(s,a)\, \frac{\sqrt{N(s)}}{1 + N(s,a)}\, ,
\quad
\widehat{Q}(s,a) \,=\, \dfrac{W(s,a)}{N(s,a)}\,
\end{equation}

\noindent gdzie:
\begin{description}
  \item[$\widehat{Q}$] - średnia wartość akcji $a$ w stanie $s$
  \item[$N(s)$] - liczba odwiedzin stanu $s$
  \item[$N(s,a)$] - liczba odwiedzeń akcji $a$ w stanie $s$
  \item[$P(s,a)$] - prawdopodobieństwo wyboru akcji $a$ w stanie $s$ zwracane przez sieć neuronową
  \item[$c_{puct}$] - współczynnik eksploracji
  \item[$W(s,a)$] - wartość sumy nagród akcji $a$ w stanie $s$
\end{description}

\hspace{2cm}

Dodatkowo ze względu na fakt, że szachy są grą dla dwóch graczy o przeciwnych celach, podczas wstecznej propagacji wartość zwrócona przez sieć value jest każdorazowo odwracana przy przejściu przez węzeł. Podobnie jest podczas obliczania funckji PUCT, gdzie składnik eksloatacji jest odwracany, gdyż z perspektywy węzła rodzica wartość ta musi być minimalizowana, a nie maksymalizowana. Oprócz odwracania, składnik jest normalizowany do przedziału [0,1].

Operacja odwracania oraz normalizowania składnika eksploatacji jest wykonywana według wzoru:
\begin{equation}
\widehat{Q}(s,a) \,=\,
\begin{cases}
0, & N(s,a) = 0 \\
1 - \dfrac{Q(s,a) + 1}{2}, & N(s,a) > 0
\end{cases}
\end{equation}

\section{WDL}
W algorytmie MCTS używanym w AlphaZero, wartość z value network po przepuszczeniu przez funkcję aktywacji tangensa hiperbolicznego, określa bezpośrednio wartość stanu gry. Jednakże istnieje lepszy sposób na oszacowanie wartości pozycji, który jest wykorzystywany w nowoczesnych silnikach takich jak LeelaChessZero \cite{lc0_wdl}. Zamiast przewidywać bezpośrednio wartość stanu, sieć może zwracać rozkład prawdopodobieństwa wszystkich możliwych wyników rozgrywki: wygrana, remis, przegrana. Taki sposób daje szerszy pogląd na sytuację, gdzie między innymi pozbywamy się problemu związanego z niejednoznacznością zera, które może oznaczać zarówno remis, jak i niejasną pozycję, gdzie żadna ze stron nie ma wyraźnej przewagi.

W celu zintegrowania takiego podejścia z MCTS, konieczne jest przekształcenie trzech wartości zwracanych przez sieć na jedną wartość liczbową. W tym celu jest wykonywany iloczyn skalarny wektora prawdopodobieństw:
\begin{equation}
V(s) = P_{win} \cdot 1 + P_{draw} \cdot 0 + P_{loss} \cdot (-1) = P_{win} - P_{loss}
\end{equation}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{images/WDL.png}
\caption{Wykres rozkładu WDL podczas trwania partii szachowej. Źródło: Leela Chess Zero, Win-Draw-Loss evaluation}
\end{figure}

\newpage

\section{First play urgency}
W podstawowej wersji MCTS podczas etapu selekcji priorytet wyboru mają nieodwiedzone węzły, które są wybierane losowo. First play urgency (FPU) zmienia tę strategię poprzez zastąpienie wartości eksploatacji wartością domyślną.

Istnieje kilka wariantów strategii FPU. Ogólna zasada zakłada, że wraz ze wzrostem tej wartości nieodwiedzone węzły są bardziej faworyzowane, gdyż może ona przykrywać wartość eksploatacji węzłów już odwiedzonych.

W przedstawianym algorytmie została użyta strategia "qFPU", w której wartość domyślna jest zastępowana wartością oczekiwaną węzła rodzica. W ten sposób nieodwiedzone węzły są faworyzowane, ale nie na tyle, aby całkowicie przykryć węzły już odwiedzone. W porównaniu do strategii ze stałą wartością jest ona znacznie bardziej adaptacyjna i dynamicznie dostosowuje się do sytuacji \cite{Cazenave2021}. 

Dla przykładu, w sytuacji gdy obecna pozycja jest niekorzystna (w skutek czego FPU przyjmie niską wartość), algorytm nabierze charakteru mocno eksploatacyjnego. Będzie on unikał ryzykownych gałęzi do momentu znalezienia ruchu poprawiającego pozycję. Jeżeli taki ruch zostanie znaleziony, algorytm zacznie go wyraźnie faworyzować. Dzięki takiemu podejściu aktywnie szuka ucieczki z trudnej sytuacji, zamiast tracić czas na eksplorację potencjalnie jeszcze gorszych ruchów, jak miałoby to miejsce w przypadku stałej, wysokiej wartości FPU. 

Natomiast jeżeli FPU będzie miało wysoką dodatnią wartość, algorytm skupi się na eksploracji, aby zwiększyć szansę na znalezienie ruchu potencjalnie jeszcze lepszego niż dotychczas znaleziony.