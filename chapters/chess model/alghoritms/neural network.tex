\section*{Połączenie MCTS z siecią neuronową}
Algorytm Monte Carlo Tree Search w podstawowej wersji korzysta z naiwnej metody oceniającej obecny stan gry, poprzez symulację losowych ruchów aż do zakończenia gry. W grach takich jak kółko i krzyżyk, gdzie przestrzeń stanów jest bardzo ograniczona, jest to wystarczające rozwiązanie i przy odpowiedniej liczbie iteracji jest skuteczne. Natomiast w grach o dużej złożoności, jak przedstawiane szachy taki sposób oceny staje się niewystarczający. Rozwiązaniem tego problemu jest połączenie MCTS z siecią neuronową, która będzie w stanie ocenić obecny stan gry oraz przedstawić rozkład prawdopodobieństwa wszystkich następnych ruchów. Takie podejście wymaga stworzenia dwóch oddzielnych sieci, lub jednej z dwoma rozgałęzieniami. Jak było wspomniane wczęsniej, jedna z nich będzie odpowiedzialna za ocenę obecnego stanu gry (value network), a druga za wybór najlepszego ruchu (policy network).

% https://www.youtube.com/watch?v=OPgRNY3FaxA
\section*{Architektura sieci neuronowej}



\section*{Implementacja MCTS oraz integracja z siecią neuronową}
W celu integracji MCTS z siecią neuronową, funkcja UCT została zastąpioną PUCT (Predictor + UCT). Takie samo podejście zostało zastosowane w AlphaZero. Różnica między UCT a PUCT występuje w obliczaniu eksploracji, gdzie między innymi jest uwzględniana wartość prawdopodobieństwa ruchu $P(s,a)$ oraz prawdopodobieństwa wygranej (wartość stanu) $W(s,a)$ zwracane przez sieć neuronową.

\hspace{2cm}

\begin{equation}
\operatorname{PUCT}(s,a) \,=\, \widehat{Q}(s,a) \, + \, c_{\mathrm{puct}}\, P(s,a)\, \frac{\sqrt{\sum_{b} N(s,b)}}{1 + N(s,a)}\, ,
\quad
\widehat{Q}(s,a) \,=\, \dfrac{W(s,a)}{N(s,a)}\,
\end{equation}

\noindent gdzie:
\begin{description}
  \item[$\widehat{Q}$] - średnia wartość akcji $a$ w stanie $s$
  \item[$N(s)$] - liczba odwiedzin stanu $s$
  \item[$N(s,a)$] - liczba odwiedzeń akcji $a$ w stanie $s$
  \item[$\sum_{b} N(s,b)$] - suma odwiedzin wszystkich akcji w stanie $s$
  \item[$P(s,a)$] - prawdopodobieństwo wyboru akcji $a$ w stanie $s$ zwracane przez sieć neuronową
  \item[$c_{puct}$] - współczynnik eksploracji
  \item[$W(s,a)$] - wartość sumy nagród akcji $a$ w stanie $s$
\end{description}

\hspace{2cm}

Dodatkowo składnik eksploatacji jest dodatkowo normalizowany i odwracany. Odwrócenie jest konieczne ze względu na fakt, że wartość jest zawsze liczona dla przeciwnika niezależnie od tego czy obecenie jest tura białych czy czarnych. Z tego powodu zależy nam na minimalizacji tej wartości, tak aby osłabiać przeciwnika. Normalizacja do przedziału [0,1] jest konieczna ze względu na fakt, że wartość $Q$ w przypadku przeważającej liczby przegranych partii może przyjmować wartości ujemne. Dodatkowo przedział [0,1] jest bardziej intuicyjny i łatwiejszy do połączenia z wartością eksploracji.

\begin{equation}
\widehat{Q}(s,a) \,=\,
\begin{cases}
0, & N(s,a) = 0 \\
1 - \dfrac{Q(s,a) + 1}{2}, & N(s,a) > 0
\end{cases}
\end{equation}